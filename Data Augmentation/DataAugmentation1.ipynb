{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "applied-abortion",
   "metadata": {},
   "source": [
    "# Pytorch try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adopted-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dasha's path\n",
    "import os\n",
    "\n",
    "folder_path = (r'C:\\Users\\darja\\Documents\\TuE\\elastix-5.0.1-win64\\TrainingData')\n",
    "elastix_path = os.path.join(r'C:\\Users\\darja\\Documents\\TuE\\elastix-5.0.1-win64\\elastix-5.0.1-win64\\elastix.exe')\n",
    "transformix_path = os.path.join(r'C:\\Users\\darja\\Documents\\TuE\\elastix-5.0.1-win64\\elastix-5.0.1-win64\\transformix.exe')\n",
    "\n",
    "slice = 86 # insert the amount of slices we work with\n",
    "patientnr = [102, 107, 108, 109, 115, 116, 117, 119, 120, 125, 127, 128, 129, 133, 135]\n",
    "#patientnr = [p102]\n",
    "\n",
    "for j in patientnr:\n",
    "    data_path=os.path.join(folder_path,'p{}'.format(j))   #it just show p135 but it is alright\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "hispanic-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\Desktop\\Uni Utrecht\\Capita Selecta\\Project\\TrainingData\n"
     ]
    }
   ],
   "source": [
    "# Giulia's path\n",
    "import os\n",
    "\n",
    "folder_path = (r'C:\\Users\\giuli\\Desktop\\Uni Utrecht\\Capita Selecta\\Project\\TrainingData')\n",
    "ELASTIX_PATH = os.path.join(r\"C:\\Users\\giuli\\Elastix\\elastix.exe\")\n",
    "TRANSFORMIX_PATH = os.path.join(r\"C:\\Users\\giuli\\Elastix\\transformix.exe\")\n",
    "\n",
    "slice = 86 # insert the amount of slices we work with\n",
    "                                \n",
    "patientnr = [102, 107, 108, 109, 115, 116, 117, 119, 120, 125, 127, 128, 129, 133, 135]\n",
    "#patientnr = 102\n",
    "data_path=os.path.join(folder_path)\n",
    "print(data_path) \n",
    "\n",
    "#i=2; \n",
    "#mask_path=  os.path.join(folder_path, 'p{}\\mr_bffe.mhd'.format(patientnr[i]))  #it works\n",
    "#print(mask_path)\n",
    "\n",
    "#for j in patientnr:\n",
    "#    data_path=os.path.join(folder_path,'p{}'.format(j))  \n",
    "#    print(data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "intellectual-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in c:\\users\\giuli\\anaconda3\\envs\\capitaselecta\\lib\\site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "# First, we import PyTorch and NumPy\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "# For the augmentations\n",
    "import torchvision\n",
    "import random\n",
    "# These two extra for evaluation.\n",
    "import difflib\n",
    "import scipy.spatial\n",
    "# We import glob to find everything that matches a pattern\n",
    "from glob import glob\n",
    "# We install and import SimpleITK for image loading\n",
    "# pip is the package installer for python\n",
    "!pip install SimpleITK\n",
    "import SimpleITK as sitk\n",
    "# To show data, we import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # progressbar "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-taxation",
   "metadata": {},
   "source": [
    "### Data Augmentations\n",
    "##### Add the augmentation so that for a specific type of augmentation the same augmentation is done for both the mask and the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "challenging-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "class transforms(): \n",
    "  def normalize(self, img, x_min, x_max): \n",
    "    max_img = torch.max(img)\n",
    "    min_img = torch.min(img)\n",
    "    nom = (img - min_img) * (x_max - x_min)\n",
    "    denom = max_img - min_img\n",
    "    denom = denom + (denom == 0) \n",
    "    return x_min + nom / denom \n",
    "\n",
    "  def rotate(self, img, mask, degrees): \n",
    "    \"\"\" Function to rotate both the image and mask with a random rotation in the same way.\n",
    "    The degrees paramater has to be passed as a range e.g. (-18, 18).\n",
    "    \"\"\"\n",
    "    angle = torchvision.transforms.RandomRotation.get_params(degrees)\n",
    "    rotated_img = torchvision.transforms.functional.rotate(img, angle)\n",
    "    rotated_mask = torchvision.transforms.functional.rotate(mask, angle)\n",
    "    return rotated_img, rotated_mask\n",
    "\n",
    "  def flip(self, img, mask): # Check if it properly works\n",
    "    flipped_img = torchvision.transforms.functional.hflip(img = img) # change to .vflip for vertical flip\n",
    "    flipped_mask = torchvision.transforms.functional.hflip(img = mask)\n",
    "    return flipped_img, flipped_mask\n",
    "\n",
    "  def scale(self, img, mask, range=0.2): # Check if it properly works\n",
    "    \"\"\"\n",
    "    Function to scale both the image and the mask mask with a random range in the same way\n",
    "    The range parameter is a float that will create a scaled image in the range of 1+- range\n",
    "    has not yet been checked to see if it works\n",
    "    \"\"\"\n",
    "    scale = random.randrange((1-range)*1000, (1+range)*1000)/1000\n",
    "    scaled_img = torchvision.transforms.functional.affine(img=img, angle=0, translate=[0,0], shear=0, scale=scale)\n",
    "    scaled_mask = torchvision.transforms.functional.affine(img=mask, angle=0, translate=[0,0], shear=0, scale=scale)\n",
    "    return scaled_img, scaled_mask\n",
    "\n",
    "  def shear(self, img, mask, degrees): # Check if it properly works.\n",
    "    degree = np.random.randint(-degrees, degrees)\n",
    "    sheared_img = torchvision.transforms.functional.affine(img = img, shear = [degree],\n",
    "                                                         angle = 0, translate = [0,0], scale = 1)\n",
    "    sheared_mask = torchvision.transforms.functional.affine(img = mask, shear = [degree],\n",
    "                                                         angle = 0, translate = [0,0], scale = 1)\n",
    "    return sheared_img, sheared_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "adapted-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maskDataset(torch.utils.data.Dataset):\n",
    "  # Datasets in Pytorch are classes of the torch.utils.data.Dataset type\n",
    "  # They __must__ have at least three methods:\n",
    "  # - __init__ -> Initialize the dataset, the place where you can pass parameters to it\n",
    "  # - __len__ -> How many samples does your dataset represent?\n",
    "  # - __getitem__ -> A function which takes a parameter i, and returns the ith sample of the dataset\n",
    "\n",
    "  # Note that this DOES NOT perform\n",
    "  # - Batching\n",
    "  # - Asynchronous dataloading (for speed)\n",
    "  # - Merge different datasets on the fly \n",
    "  # - shuffling the data\n",
    "  # More examples like these are solved with \"higher-order\" methods\n",
    "\n",
    "  # but it __might__ do:\n",
    "  # - data augmentation of one sample\n",
    "  # - data normalization of one sample\n",
    "  # - performing on-the-fly data generation\n",
    "  # - hides the nitty-gritty details of dealing with files\n",
    "    \n",
    "  def __init__(self, data_path, normalize = False, rotate = (False, 0) , flip = False, scale = False, shear = (False, 0)):\n",
    "    \n",
    "    self.slices = slice \n",
    "    self.datafolder = glob(data_path)\n",
    "    print(\"self.datafolder is: \", self.datafolder)\n",
    "    print(\"patientnr\", patientnr)\n",
    "\n",
    "    # Initializations for data augmentation\n",
    "    self.transforms = transforms()\n",
    "\n",
    "    # I'd suggest passing any extra parameters necessary for the transformation along with the variable as a tuple.\n",
    "    # Then unpack the tuple here and use it later, when applying the augmentation. This way those parameters are not fixed inside the class.\n",
    "    self.normalize = normalize\n",
    "    self.rotate, self.rotation_angle = rotate\n",
    "    self.flip = flip\n",
    "    self.scale = scale\n",
    "    self.shear, self.shear_angle = shear\n",
    "\n",
    "  def __len__(self): # the length is the number of patients scanned at the institute. Every patient is a subfolder in the institute folder\n",
    "    return len(self.datafolder) * self.slices\n",
    "\n",
    "  # This is a helper function to avoid repeating the same SimpleITK function calls to load the images\n",
    "  # It loads the Nifti files, gets a correctly spaced NumPy array, and creates a tensor\n",
    "  def read_image(self, path):\n",
    "    img = sitk.ReadImage(path)\n",
    "    img_as_numpy = sitk.GetArrayFromImage(img).astype('float') # the default type is uint16, which trips up PyTorch so we convert to float\n",
    "    img_as_tensor = torch.from_numpy(img_as_numpy)\n",
    "    return img_as_tensor\n",
    "\n",
    "  def __getitem__(self, i): # return the ith sample of the dataset, note that 0 <= i < len(dataset)\n",
    "    # A slice is considered a sample.\n",
    "     \n",
    "    print(\"patientnrorI\", patientnr[i]) #it works\n",
    "    \n",
    "    \n",
    "    mask_path=  os.path.join(self.datafolder, 'p{}\\mr_bffe.mhd'.format(patientnr[i]))\n",
    "    img_path =  os.path.join(self.datafolder, 'p{}\\prostaat.mhd'.format(patientnr[i]))\n",
    "    print(\"mask_path is: \", mask_path)\n",
    "    \n",
    "    #mask_path = f'{self.datafolder}\\\\mr_bffe.mhd'\n",
    "    #img_path = f'{self.datafolder[patientnr]}/prostaat.mhd'\n",
    "    #img_path = f'{self.datafolder}\\\\prostaat.mhd'\n",
    "    \n",
    "    # Actually load the Nifti files and create PyTorch tensors\n",
    "    mask = self.read_image(mask_path)\n",
    "    img = self.read_image(img_path)\n",
    "  \n",
    "    _, x, y = mask.size()\n",
    "    train_tensor = torch.zeros((1, x, y)) # Use only one to avoid error shown.\n",
    "    target_tensor = torch.zeros((1, x, y))\n",
    "    \n",
    "    slice_index = i % self.slices\n",
    "    train_tensor[0, ...] = img[slice_index, ...]\n",
    "    target_tensor[0, ...] = mask[slice_index, ...]\n",
    "\n",
    "    # Apply normalization\n",
    "    if self.normalize:\n",
    "      train_tensor = self.transforms.normalize(train_tensor, 0, 1)\n",
    "\n",
    "    # Apply data augmentation\n",
    "    if self.rotate:\n",
    "      train_tensor, target_tensor = self.transforms.rotate(train_tensor, target_tensor, self.rotation_angle)\n",
    "    \n",
    "    if self.flip:\n",
    "      train_tensor, target_tensor = self.transforms.flip(train_tensor, target_tensor)\n",
    "    \n",
    "    if self.scale:\n",
    "      train_tensor, target_tensor = self.transforms.scale(train_tensor, target_tensor)\n",
    "    \n",
    "    if self.shear:\n",
    "      train_tensor, target_tensor = self.transforms.shear(train_tensor, target_tensor, self.shear_angle)\n",
    "\n",
    "    # Return the samples as PyTorch tensors\n",
    "    return train_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "adapted-simple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.datafolder is:  ['C:\\\\Users\\\\giuli\\\\Desktop\\\\Uni Utrecht\\\\Capita Selecta\\\\Project\\\\TrainingData']\n",
      "patientnr [102, 107, 108, 109, 115, 116, 117, 119, 120, 125, 127, 128, 129, 133, 135]\n",
      "['C:\\\\Users\\\\giuli\\\\Desktop\\\\Uni Utrecht\\\\Capita Selecta\\\\Project\\\\TrainingData']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Load data\n",
    "\n",
    "dataset = maskDataset(data_path, 2) # Note this dataset is now already normalized.\n",
    "print(dataset.datafolder) \n",
    "len(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "defined-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patientnrorI 102\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-ddc881ffcd31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Take a look at what the data looks like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-45bcb39d1e12>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mmask_path\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatafolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'p{}\\mr_bffe.mhd'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatientnr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatafolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'p{}\\prostaat.mhd'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatientnr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mask_path is: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CapitaSelecta\\lib\\ntpath.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# Join two (or more) paths.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb'\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "#Take a look at what the data looks like\n",
    "train, target = dataset[0]\n",
    "train.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-acoustic",
   "metadata": {},
   "source": [
    "# Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "showFig = 'yes'\n",
    "\n",
    "if showFig=='yes':\n",
    "  f, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "\n",
    "  # turn off axis to remove ticks and such\n",
    "  [a.axis('off') for a in ax]\n",
    "\n",
    "  # Here we plot it at the actual subplot we want. We set the colormap to gray (feel free to experiment)\n",
    "  img_plot = ax[0].imshow(train[0, :, :], cmap='gray') # Was one but we only working with flair for now.\n",
    "  mask_plot = ax[1].imshow(target[0, :, :], cmap='gray')\n",
    "\n",
    "  # Add titles and colorbar\n",
    "  ax[0].set_title('Image')\n",
    "  ax[1].set_title('Previously provided mask')\n",
    "\n",
    "  f.colorbar(img_plot, ax=ax[0], shrink=0.25)\n",
    "  f.colorbar(mask_plot, ax=ax[1], shrink=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-spine",
   "metadata": {},
   "source": [
    "# Model setting & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # Basic building blocks (containers, different layers ect.)\n",
    "\n",
    "def double_conv(in_channels, out_channels): \n",
    "    return nn.Sequential( # a sequential container, elements are kept in order\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        # 3 is the size of the kernel? Padding is zero padding  \n",
    "        nn.ReLU(inplace=True),\n",
    "        # Activation function ReLU = applies the rectified linear unit function element wise \n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "  # nn.Module is the base class for all neural network modules\n",
    "\n",
    "    def __init__(self, n_class, n_channels):\n",
    "      #Initialization, number of classes, number of channels\n",
    "        super().__init__()\n",
    "        # The U-shape         \n",
    "        self.dconv_down1 = double_conv(n_channels, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2) # a 2x2 kernel \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, lossfunction, device='cuda'):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  for batch_idx, (train, target) in enumerate(tqdm(train_loader)):\n",
    "    train, target = train.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = lossfunction(target, output)\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader, eval_function, device='cuda'):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (train, target) in enumerate(tqdm(test_loader)):\n",
    "      train, target = train.to(device), target.to(device) \n",
    "      output = model(train)\n",
    "      loss = eval_function(target, output)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "  return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, optimizer, lossfunction, eval_function=None, epochs=25, device='cuda'):\n",
    "  if eval_function is None:\n",
    "    eval_function = lossfunction\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  for epoch in range(epochs):\n",
    "    print('[Epoch %d / %d]' % (epoch + 1, epochs))\n",
    "    train_loss = train_epoch(model, optimizer, train_loader, lossfunction, device)\n",
    "    test_loss = test_epoch(model, test_loader, eval_function, device)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "  return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device='cuda'\n",
    "model = UNet(1,1)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-stanford",
   "metadata": {},
   "source": [
    "# Inceasing Dataset with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications on dataset should come bout here, in this and next cell.\n",
    "dataset_all_transforms = maskDataset('Utrecht', rotate = (True, (-18, 18)), shear=(True, 30), scale =True, flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-exchange",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
